{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cb6648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] S=50, T=100, C=1, H×W=28×28\n",
      "X_test: shape=(50, 100, 1, 28, 28), dtype=torch.float32, device=cpu, min=0, max=0.001953\n",
      "[mask] None (metrics computed over all frames)\n",
      "[Zero | raw] mean=0.207 ×10^3, std=0.003, sem=0.000\n",
      "[Saved DI | raw] mean=0.207 ×10^3, std=0.003, sem=0.000\n",
      "[info] No Z provided; skip calibrated sigmoid reporting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Double Pendulum Metrics Script (robust & shape-safe)\n",
    "# - per_frame_mae: safe MAE that aligns C,H,W and handles masks\n",
    "# - summarize_x1e3: mean/std/sem × 1000 reporting\n",
    "# - Optional: infer G from checkpoint, decode & resize\n",
    "# Author: M365 Copilot (Esteban-tailored)\n",
    "# ============================================================\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# Diagnostics & small utilities\n",
    "# -------------------------------\n",
    "\n",
    "def describe_tensor(name: str, t: torch.Tensor):\n",
    "    if t is None:\n",
    "        print(f\"{name}: None\")\n",
    "        return\n",
    "    shp = tuple(t.shape)\n",
    "    dev = t.device\n",
    "    dt  = t.dtype\n",
    "    msg = f\"{name}: shape={shp}, dtype={dt}, device={dev}\"\n",
    "    try:\n",
    "        tmin = t.min().item()\n",
    "        tmax = t.max().item()\n",
    "        msg += f\", min={tmin:.4g}, max={tmax:.4g}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(msg)\n",
    "\n",
    "def ensure_channel_dim(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Ensure tensor is [S,T,1,H,W] if it was [S,T,H,W].\"\"\"\n",
    "    if x.dim() == 4:\n",
    "        return x.unsqueeze(2)\n",
    "    return x\n",
    "\n",
    "# ----------------------------------------\n",
    "# Metrics: MAE per-frame (shape-safe)\n",
    "# ----------------------------------------\n",
    "\n",
    "def per_frame_mae(\n",
    "    xh: torch.Tensor,\n",
    "    xg: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "    resize_mode: str = 'bilinear',\n",
    "    align_corners: bool = False,\n",
    "    flatten: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute per-frame MAE between predicted xh and ground-truth xg.\n",
    "    Inputs:\n",
    "      xh, xg: [S,T,C,H,W] or [S,T,H,W] tensors. We will enforce [S,T,C,H,W].\n",
    "      mask: optional boolean [S,T]; selects valid frames. If empty, returns empty tensor (not None).\n",
    "      resize_mode: interpolation mode if H×W mismatch; 'bilinear'|'bicubic'|'nearest'.\n",
    "      align_corners: passed to interpolate for bilinear/bicubic.\n",
    "      flatten: if True -> return vector [N_masked]; else -> [S,T] (masked frames removed or zeroed).\n",
    "\n",
    "    Returns:\n",
    "      Tensor: per-frame MAE either [S,T] or [N]. Never returns None.\n",
    "    \"\"\"\n",
    "    # Normalize shapes to [S,T,1,H,W]\n",
    "    assert xh.dim() in (4, 5) and xg.dim() in (4, 5), \"Expected 4D/5D tensors\"\n",
    "    xh = ensure_channel_dim(xh)\n",
    "    xg = ensure_channel_dim(xg)\n",
    "\n",
    "    # Device/dtype alignment\n",
    "    xh = xh.to(device=xg.device, dtype=xg.dtype)\n",
    "\n",
    "    # Channel alignment (common case: C=1). If mismatch, try expanding singleton.\n",
    "    if xh.size(2) != xg.size(2):\n",
    "        if xh.size(2) == 1 and xg.size(2) > 1:\n",
    "            xh = xh.expand(-1, -1, xg.size(2), -1, -1)\n",
    "        elif xg.size(2) == 1 and xh.size(2) > 1:\n",
    "            xg = xg.expand(-1, -1, xh.size(2), -1, -1)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Channel mismatch: xh C={xh.size(2)} vs xg C={xg.size(2)}\")\n",
    "\n",
    "    # Spatial alignment: resize xh to xg geometry (safe for metrics)\n",
    "    Hg, Wg = xg.size(-2), xg.size(-1)\n",
    "    Hh, Wh = xh.size(-2), xh.size(-1)\n",
    "    if (Hh, Wh) != (Hg, Wg):\n",
    "        xh = F.interpolate(\n",
    "            xh.reshape(-1, xh.size(2), Hh, Wh),\n",
    "            size=(Hg, Wg),\n",
    "            mode=resize_mode,\n",
    "            align_corners=(align_corners if resize_mode in ('bilinear', 'bicubic') else None)\n",
    "        ).reshape(xg.size(0), xg.size(1), xg.size(2), Hg, Wg)\n",
    "\n",
    "    # MAE over C,H,W -> [S,T]\n",
    "    diff = (xh - xg).abs().mean(dim=(2, 3, 4))\n",
    "\n",
    "    # Masking\n",
    "    if mask is not None:\n",
    "        if mask.dtype != torch.bool:\n",
    "            mask = mask.bool()\n",
    "        if mask.shape != diff.shape:\n",
    "            raise RuntimeError(f\"Mask shape mismatch: mask {tuple(mask.shape)} vs diff {tuple(diff.shape)}\")\n",
    "        diff_masked = diff[mask]\n",
    "        if diff_masked.numel() == 0:\n",
    "            # Return empty tensor (caller-friendly)\n",
    "            return diff_masked\n",
    "        return diff_masked if flatten else diff * mask\n",
    "\n",
    "    return diff.reshape(-1) if flatten else diff\n",
    "\n",
    "# ----------------------------------------\n",
    "# Reporting: mean/std/sem × 1000\n",
    "# ----------------------------------------\n",
    "\n",
    "def summarize_x1e3(vals: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Return (mean, std, sem) scaled by 1000 for reporting.\n",
    "    Handles None and empty tensors gracefully.\n",
    "    \"\"\"\n",
    "    if vals is None:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    if not torch.is_tensor(vals):\n",
    "        vals = torch.as_tensor(vals)\n",
    "    n = vals.numel()\n",
    "    if n == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    mean = vals.mean().item()\n",
    "    std  = vals.std(unbiased=True).item() if n > 1 else 0.0\n",
    "    sem  = (std / (n ** 0.5)) if n > 1 else 0.0\n",
    "    return mean * 1000.0, std * 1000.0, sem * 1000.0\n",
    "\n",
    "# ----------------------------------------\n",
    "# Optional: build G from checkpoint safely\n",
    "# ----------------------------------------\n",
    "\n",
    "def infer_generator_from_checkpoint(state: dict):\n",
    "    \"\"\"\n",
    "    Infer a 4-layer MLP generator G from a state dict with keys:\n",
    "      'first_layer.weight'   # [h1, d]\n",
    "      'second_layer.weight'  # [h2, h1]\n",
    "      'third_layer.weight'   # [h3, h2]\n",
    "      'fourth_layer.weight'  # [x_dim, h3]\n",
    "    Returns:\n",
    "      (G_model, latent_dim, x_dim, H_ckpt, W_ckpt)\n",
    "    \"\"\"\n",
    "    w1 = state['first_layer.weight']   # [h1, d]\n",
    "    w2 = state['second_layer.weight']  # [h2, h1]\n",
    "    w3 = state['third_layer.weight']   # [h3, h2]\n",
    "    w4 = state['fourth_layer.weight']  # [x_dim, h3]\n",
    "\n",
    "    latent_dim = w1.shape[1]\n",
    "    h1 = w1.shape[0]\n",
    "    h2 = w2.shape[0]\n",
    "    h3 = w3.shape[0]\n",
    "    x_dim = w4.shape[0]\n",
    "\n",
    "    class G(nn.Module):\n",
    "        def __init__(self, d, x, h1, h2, h3):\n",
    "            super().__init__()\n",
    "            self.first_layer  = nn.Linear(d,  h1)\n",
    "            self.second_layer = nn.Linear(h1, h2)\n",
    "            self.third_layer  = nn.Linear(h2, h3)\n",
    "            self.fourth_layer = nn.Linear(h3, x)\n",
    "\n",
    "        def forward(self, z):\n",
    "            x = torch.tanh(self.first_layer(z))\n",
    "            x = torch.tanh(self.second_layer(x))\n",
    "            x = torch.tanh(self.third_layer(x))\n",
    "            x = self.fourth_layer(x)  # [*, x_dim]\n",
    "            return x\n",
    "\n",
    "    G_model = G(latent_dim, x_dim, h1, h2, h3)\n",
    "    G_model.load_state_dict(state)\n",
    "    G_model.eval()\n",
    "\n",
    "    # Infer square image size if possible\n",
    "    r = int(math.isqrt(x_dim))\n",
    "    H_ckpt, W_ckpt = (r, r) if r * r == x_dim else (None, None)\n",
    "\n",
    "    return G_model, latent_dim, x_dim, H_ckpt, W_ckpt\n",
    "\n",
    "def decode_and_resize(\n",
    "    y_flat: torch.Tensor,\n",
    "    H_src: int, W_src: int,\n",
    "    H_dst: int, W_dst: int,\n",
    "    mode: str = 'bilinear'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape flat outputs [..., x_dim] to image [..., 1, H_src, W_src],\n",
    "    then resize to [..., 1, H_dst, W_dst].\n",
    "    \"\"\"\n",
    "    if H_src is None or W_src is None:\n",
    "        raise RuntimeError(f\"Cannot reshape: x_dim={y_flat.shape[-1]} is not a perfect square.\")\n",
    "    y_img = y_flat.view(*y_flat.shape[:-1], 1, H_src, W_src)\n",
    "    if (H_src, W_src) != (H_dst, W_dst):\n",
    "        y_img = F.interpolate(\n",
    "            y_img.reshape(-1, 1, H_src, W_src),\n",
    "            size=(H_dst, W_dst),\n",
    "            mode=mode,\n",
    "            align_corners=False if mode in ('bilinear','bicubic') else None\n",
    "        ).reshape(*y_flat.shape[:-1], 1, H_dst, W_dst)\n",
    "    return y_img\n",
    "\n",
    "# ----------------------------------------\n",
    "# Optional: sigmoid mapping g(Z) (calib)\n",
    "# ----------------------------------------\n",
    "\n",
    "def sigmoid(u: torch.Tensor) -> torch.Tensor:\n",
    "    return 1.0 / (1.0 + torch.exp(-u))\n",
    "\n",
    "def g_of_Z(\n",
    "    Z: torch.Tensor,\n",
    "    baseline_k: float = 147_000.0,  # paper zero baseline\n",
    "    amplitude_k: float = 300_000.0, # placeholder; fit to your data\n",
    "    k: float = 1.0,\n",
    "    Z0: float = 0.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    counts(k) = baseline + amplitude * σ(k * (Z - Z0))\n",
    "    Ensure Z is in the expected domain before calling (raw vs normalized).\n",
    "    \"\"\"\n",
    "    s = sigmoid(k * (Z - Z0))\n",
    "    return baseline_k + amplitude_k * s\n",
    "\n",
    "# ============================================================\n",
    "# Config & Execution (edit section below to fit your run)\n",
    "# ============================================================\n",
    "\n",
    "# Assumptions:\n",
    "# - You already have X_test and mask in memory:\n",
    "#   X_test: [S,T,H,W] or [S,T,1,H,W]\n",
    "#   mask: [S,T] boolean or None\n",
    "#\n",
    "# - PRED_X_TEST points to a torch-saved tensor of predictions (Saved DI):\n",
    "#   shape could be [S,T,H,W] or [S,T,1,H,W]\n",
    "#\n",
    "# - CKPT_PATH is optional; set it if you want to recompute from checkpoint.\n",
    "\n",
    "# ---------- EDIT THESE ----------\n",
    "PRED_X_TEST = 'checkpoints/pixel_double_pendulum/di/baseline_x_test.pkl'  # e.g., './pred_x_test.pt'\n",
    "CKPT_PATH   = None                      # e.g., './generator_ckpt.pt' or leave None\n",
    "RECOMPUTE_FROM_CHECKPOINT = False       # set True to decode with G and resize\n",
    "# ---------------------------------\n",
    "\n",
    "# Sanity: X_test must exist\n",
    "try:\n",
    "    X_test\n",
    "except NameError:\n",
    "    raise RuntimeError(\"X_test is not defined. Please set X_test before running this script.\")\n",
    "\n",
    "# Normalize X_test shape & get geometry\n",
    "X_test = ensure_channel_dim(X_test)\n",
    "S, T, C, H_data, W_data = X_test.shape\n",
    "print(f\"[data] S={S}, T={T}, C={C}, H×W={H_data}×{W_data}\")\n",
    "describe_tensor(\"X_test\", X_test)\n",
    "\n",
    "# Validate mask\n",
    "if 'mask' in globals() and mask is not None:\n",
    "    assert mask.dtype == torch.bool, f\"mask must be boolean, got {mask.dtype}\"\n",
    "    assert tuple(mask.shape) == (S, T), f\"mask shape {tuple(mask.shape)} must be {(S,T)}\"\n",
    "    print(f\"[mask] selected {mask.sum().item()} of {mask.numel()} frames\")\n",
    "else:\n",
    "    mask = None\n",
    "    print(\"[mask] None (metrics computed over all frames)\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Zero baseline (raw scale)\n",
    "# ------------------------------\n",
    "zero = torch.zeros_like(X_test)\n",
    "mae_zero_raw = per_frame_mae(zero, X_test, mask=mask, flatten=True)\n",
    "z_mean_raw, z_std_raw, z_sem_raw = summarize_x1e3(mae_zero_raw)\n",
    "print(f\"[Zero | raw] mean={z_mean_raw:.3f} ×10^3, std={z_std_raw:.3f}, sem={z_sem_raw:.3f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Saved DI predictions loaded & aligned to data\n",
    "# -------------------------------------------------\n",
    "try:\n",
    "    X_hat_saved = torch.load(PRED_X_TEST, map_location=X_test.device)\n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(f\"PRED_X_TEST not found: {PRED_X_TEST}\")\n",
    "\n",
    "X_hat_saved = ensure_channel_dim(X_hat_saved)  # -> [S,T,1,H,W] if needed\n",
    "\n",
    "# If S,T mismatch, try to reshape if flat batch was saved; else raise with guidance.\n",
    "if X_hat_saved.shape[:2] != (S, T):\n",
    "    raise RuntimeError(\n",
    "        f\"Saved predictions shape {tuple(X_hat_saved.shape[:2])} != data {(S,T)}.\\n\"\n",
    "        f\"Please ensure predictions were saved in [S,T,...] order and match the dataset.\"\n",
    "    )\n",
    "\n",
    "# Compute MAE (function will auto-resize H×W of xh to match xg)\n",
    "mae_saved_raw = per_frame_mae(X_hat_saved, X_test, mask=mask, flatten=True)\n",
    "s_mean_raw, s_std_raw, s_sem_raw = summarize_x1e3(mae_saved_raw)\n",
    "print(f\"[Saved DI | raw] mean={s_mean_raw:.3f} ×10^3, std={s_std_raw:.3f}, sem={s_sem_raw:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) Optional: recompute predictions from checkpoint G\n",
    "# ------------------------------------------------------\n",
    "if RECOMPUTE_FROM_CHECKPOINT:\n",
    "    if CKPT_PATH is None:\n",
    "        raise RuntimeError(\"RECOMPUTE_FROM_CHECKPOINT=True but CKPT_PATH is None.\")\n",
    "    # Load state\n",
    "    ckpt_obj = torch.load(CKPT_PATH, map_location=X_test.device)\n",
    "    state = ckpt_obj['state_dict'] if isinstance(ckpt_obj, dict) and 'state_dict' in ckpt_obj else ckpt_obj\n",
    "\n",
    "    # Build generator G from checkpoint shapes\n",
    "    G_model, latent_dim, x_dim, H_ckpt, W_ckpt = infer_generator_from_checkpoint(state)\n",
    "    print(f\"[ckpt] latent_dim={latent_dim}, x_dim={x_dim}, image={H_ckpt}×{W_ckpt}\")\n",
    "\n",
    "    # Construct latent Z for S×T frames (you may want to load real Z instead)\n",
    "    # Here we use a placeholder standard normal—replace with your actual latent sequence.\n",
    "    Z = torch.randn(S*T, latent_dim, device=X_test.device)\n",
    "\n",
    "    # Generate flat outputs and decode to images at checkpoint resolution\n",
    "    Y_flat = G_model(Z)  # [S*T, x_dim]\n",
    "    Y_img_resized = decode_and_resize(Y_flat, H_ckpt, W_ckpt, H_data, W_data)  # [S*T,1,H_data,W_data]\n",
    "    Y_img_resized = Y_img_resized.view(S, T, 1, H_data, W_data)\n",
    "\n",
    "    # MAE against X_test\n",
    "    mae_recomp_raw = per_frame_mae(Y_img_resized, X_test, mask=mask, flatten=True)\n",
    "    r_mean_raw, r_std_raw, r_sem_raw = summarize_x1e3(mae_recomp_raw)\n",
    "    print(f\"[Recomputed G(Z) | raw] mean={r_mean_raw:.3f} ×10^3, std={r_std_raw:.3f}, sem={r_sem_raw:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) Optional: calibrated sigmoid mapping g(Z)\n",
    "# ------------------------------------------------------\n",
    "# If you have a scalar Z per frame and want to report calibrated counts:\n",
    "# Example placeholders (replace Z, amplitude_k, k, Z0 as appropriate)\n",
    "if 'Z' in globals():\n",
    "    Z_t = torch.as_tensor(Z, device=X_test.device, dtype=torch.float32)\n",
    "    counts = g_of_Z(Z_t, baseline_k=147_000.0, amplitude_k=300_000.0, k=1.0, Z0=0.0)\n",
    "    # Summarize counts (not MAE)\n",
    "    c_mean = counts.mean().item()\n",
    "    c_std  = counts.std(unbiased=True).item() if counts.numel() > 1 else 0.0\n",
    "    c_sem  = c_std / (counts.numel() ** 0.5) if counts.numel() > 1 else 0.0\n",
    "    print(f\"[Recomputed g(Z) | calib+sigmoid] mean={c_mean/1000.0:.1f} ×10^3, std={c_std/1000.0:.1f}, sem={c_sem/1000.0:.1f}\")\n",
    "else:\n",
    "    print(\"[info] No Z provided; skip calibrated sigmoid reporting.\")\n",
    "\n",
    "# ============================================================\n",
    "# End of script\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196bff0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_test(raw): shape=(50, 100, 4), dtype=torch.float32, device=cpu, min=-3.334, max=3.609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Z_test Integration (latent for G and/or scalar for g(Z))\n",
    "# Add this block under the Config section in the previous script\n",
    "# ============================================================\n",
    "\n",
    "# ---------- EDIT THESE ----------\n",
    "Z_TEST_PATH = \"checkpoints/pixel_double_pendulum/di/baseline_z_test.pkl\"  # e.g., './Z_test.pt' (set to None if you don't have it)\n",
    "Z_KIND = 'latent'   # 'latent' for generator input; 'scalar' for calibrated sigmoid\n",
    "# ---------------------------------\n",
    "\n",
    "def load_Z_test(path, device):\n",
    "    if path is None:\n",
    "        return None\n",
    "    Z_loaded = torch.load(path, map_location=device)\n",
    "    return Z_loaded\n",
    "\n",
    "Z_test = load_Z_test(Z_TEST_PATH, X_test.device)\n",
    "\n",
    "if Z_test is None:\n",
    "    print(\"[info] No Z_test provided; generator recompute will use random Z, and calibrated sigmoid will be skipped unless Z exists elsewhere.\")\n",
    "else:\n",
    "    describe_tensor(\"Z_test(raw)\", Z_test)\n",
    "\n",
    "# -------------------------\n",
    "# Z for generator (latent)\n",
    "# -------------------------\n",
    "if RECOMPUTE_FROM_CHECKPOINT:\n",
    "    if CKPT_PATH is None:\n",
    "        raise RuntimeError(\"RECOMPUTE_FROM_CHECKPOINT=True but CKPT_PATH is None.\")\n",
    "    # Load state\n",
    "    ckpt_obj = torch.load(CKPT_PATH, map_location=X_test.device)\n",
    "    state = ckpt_obj['state_dict'] if isinstance(ckpt_obj, dict) and 'state_dict' in ckpt_obj else ckpt_obj\n",
    "\n",
    "    # Build generator G from checkpoint shapes\n",
    "    G_model, latent_dim, x_dim, H_ckpt, W_ckpt = infer_generator_from_checkpoint(state)\n",
    "    print(f\"[ckpt] latent_dim={latent_dim}, x_dim={x_dim}, image={H_ckpt}×{W_ckpt}\")\n",
    "\n",
    "    # Prepare Z for generator\n",
    "    if Z_KIND == 'latent' and Z_test is not None:\n",
    "        # Accept shapes: [S,T,latent_dim] or [S*T,latent_dim]\n",
    "        if Z_test.dim() == 3 and Z_test.shape[:2] == (S, T):\n",
    "            assert Z_test.shape[2] == latent_dim, (\n",
    "                f\"Z_test latent dim {Z_test.shape[2]} != checkpoint latent_dim {latent_dim}\"\n",
    "            )\n",
    "            Z_for_G = Z_test.reshape(S*T, latent_dim).to(X_test.device, dtype=torch.float32)\n",
    "        elif Z_test.dim() == 2 and Z_test.shape[0] == S*T and Z_test.shape[1] == latent_dim:\n",
    "            Z_for_G = Z_test.to(X_test.device, dtype=torch.float32)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Z_test shape {tuple(Z_test.shape)} is not compatible with [S,T,{latent_dim}] or [S*T,{latent_dim}]\"\n",
    "            )\n",
    "        print(f\"[Z_test] Using provided latent Z with shape {tuple(Z_for_G.shape)}\")\n",
    "    else:\n",
    "        # Fallback: random Z (warn user)\n",
    "        Z_for_G = torch.randn(S*T, latent_dim, device=X_test.device)\n",
    "        print(\"[warn] Z_test not provided or Z_KIND!='latent'; using random Z for generator recompute.\")\n",
    "\n",
    "    # Generate and resize to dataset geometry\n",
    "    Y_flat = G_model(Z_for_G)  # [S*T, x_dim]\n",
    "    Y_img_resized = decode_and_resize(Y_flat, H_ckpt, W_ckpt, H_data, W_data)  # [S*T,1,H_data,W_data]\n",
    "    Y_img_resized = Y_img_resized.view(S, T, 1, H_data, W_data)\n",
    "\n",
    "    # MAE against X_test\n",
    "    mae_recomp_raw = per_frame_mae(Y_img_resized, X_test, mask=mask, flatten=True)\n",
    "    r_mean_raw, r_std_raw, r_sem_raw = summarize_x1e3(mae_recomp_raw)\n",
    "    print(f\"[Recomputed G(Z_test) | raw] mean={r_mean_raw:.3f} ×10^3, std={r_std_raw:.3f}, sem={r_sem_raw:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Z for calibrated sigmoid g(Z)\n",
    "# -----------------------------\n",
    "if Z_KIND == 'scalar' and Z_test is not None:\n",
    "    # Accept [S,T], [S*T], or NumPy array; convert to float tensor\n",
    "    if Z_test.dim() == 2 and Z_test.shape == (S, T):\n",
    "        Z_scalar = Z_test.reshape(S*T).to(X_test.device, dtype=torch.float32)\n",
    "    elif Z_test.dim() == 1 and Z_test.shape[0] == S*T:\n",
    "        Z_scalar = Z_test.to(X_test.device, dtype=torch.float32)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Z_test shape {tuple(Z_test.shape)} must be [S,T] or [S*T] for scalar g(Z).\"\n",
    "        )\n",
    "\n",
    "    # TODO: ensure Z_scalar is in the expected domain (raw vs normalized)\n",
    "    # For now, we pass it directly.\n",
    "    counts = g_of_Z(Z_scalar, baseline_k=147_000.0, amplitude_k=300_000.0, k=1.0, Z0=0.0)\n",
    "    c_mean = counts.mean().item()\n",
    "    c_std  = counts.std(unbiased=True).item() if counts.numel() > 1 else 0.0\n",
    "    c_sem  = c_std / (counts.numel() ** 0.5) if counts.numel() > 1 else 0.0\n",
    "    print(f\"[g(Z_test) | calib+sigmoid] mean={c_mean/1000.0:.1f} ×10^3, std={c_std/1000.0:.1f}, sem={c_sem/1000.0:.1f}\")\n",
    "elif Z_KIND == 'scalar' and Z_test is None:\n",
    "    print(\"[info] Z_KIND='scalar' but Z_test not provided; skipping calibrated sigmoid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88819c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GOKU (3.13.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
